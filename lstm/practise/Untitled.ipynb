{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import reader\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件  select_description_from_sys_district.csv 存在。盛宴大幕拉开了。\n"
     ]
    }
   ],
   "source": [
    "file_name = 'select_description_from_sys_district.csv'\n",
    "if reader.check_file(file_name, url=''):\n",
    "    hotelDescs = reader.read_data_as_text(file_name)\n",
    "else:\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c4b56d045aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'select_description_from_sys_district.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_index2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_index2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('datasets','select_description_from_sys_district.csv')) as data_f:\n",
    "    trainX, trainY, testX, testY, vocabulary_index2word=pickle.load(data_f)\n",
    "    vocab_size=len(vocabulary_index2word)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件  select_description_from_sys_district.csv 存在。盛宴大幕拉开了。\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2019年开业60间房。上海时生隅园林文化酒店：2017年的秋末，时生隅初见。彼时的心愿，想要在快节奏的都市，寻一处“讲诉时光和生命的角落”。不问对错，不闻浮沉。这便是“时生隅”的由来。 这一次，我们秉承初心，在闹中取静的川沙地铁站商圈，觅得一方天地—充满江南气息的园林，叶园。长廊湖景，古雅庭院。这里淡雅而大气，是快节奏与民俗文化的交融，这里幽静而明快，是远道而来亦或小憩而居的别样天地。时生隅的新老朋友，我们在叶园，期待与您共襄当下景，把时光言欢。'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-668637cea42b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'select_description_from_sys_district.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;31m# converting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mfloatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'0x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2019年开业60间房。上海时生隅园林文化酒店：2017年的秋末，时生隅初见。彼时的心愿，想要在快节奏的都市，寻一处“讲诉时光和生命的角落”。不问对错，不闻浮沉。这便是“时生隅”的由来。 这一次，我们秉承初心，在闹中取静的川沙地铁站商圈，觅得一方天地—充满江南气息的园林，叶园。长廊湖景，古雅庭院。这里淡雅而大气，是快节奏与民俗文化的交融，这里幽静而明快，是远道而来亦或小憩而居的别样天地。时生隅的新老朋友，我们在叶园，期待与您共襄当下景，把时光言欢。'"
     ]
    }
   ],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64) \n",
    "\n",
    "if not os.path.exists(os.path.join('datasets',file_name)):\n",
    "    print('文件',file_name, '不存在，请确保文件在datasets目录下。')\n",
    "    #file_name = urlretrieve(url + file_name, dir_name + os.sep + file_name)\n",
    "else:\n",
    "    print('文件 ',file_name, '存在。盛宴大幕拉开了。')\n",
    "\n",
    "\n",
    "data=np.loadtxt(os.path.join('datasets','select_description_from_sys_district.csv'),delimiter=',')\n",
    "X_train=data[:,0]\n",
    "y_train=data[:,1]\n",
    "print(X_train)\n",
    "# def readMyFileFormat(fileNameQueue):\n",
    "#     reader = tf.compat.v1.TextLineReader()\n",
    "#     key, value = reader.read(fileNameQueue)\n",
    "#     record_defaults = [[1], [1], [1]]\n",
    "#     col1, col2, col3 = tf.decode_csv(value, record_defaults = record_defaults)\n",
    "#     features = tf.pack([col1, col2])\n",
    "#     label = col3\n",
    "#     return features, label\n",
    "\n",
    "# example, label = readMyFileFormat([os.path.join('datasets','select_description_from_sys_district.csv')])\n",
    "\n",
    "# labeled_data_sets = []\n",
    "# lines_dataset = tf.data.TextLineDataset(os.path.join('datasets','select_description_from_sys_district.csv'))\n",
    "# x_train, y_label = tf.compat.v1.decode_csv(hotelDescs, record_defaults=tf.int64)\n",
    "\n",
    "\n",
    "# print(y_label)\n",
    "# labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, 1))\n",
    "# labeled_data_sets.append(labeled_dataset)\n",
    "# tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "# vocabulary_set = set()\n",
    "# for text_tensor, _ in all_labeled_data:\n",
    "#   some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "#   vocabulary_set.update(some_tokens)\n",
    "\n",
    "# vocab_size = len(vocabulary_set)\n",
    "# vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n",
      "819200/815980 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n",
      "811008/809730 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n",
      "811008/807992 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/.keras/datasets'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>, <MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>, <MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>]\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'daughter of mighty Saturn, say what you want, and I will do it for you'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'In yon rude host those noblest of the Greeks.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"Then Hector to his comrades' ranks withdrew,\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'while his son Dolops was versed in all the ways of war. He then struck'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Here to abide in prospect of the fleet,'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "def labeler(example, index):\n",
    "  return example, tf.cast(index, tf.int64)  \n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "  labeled_data_sets.append(labeled_dataset)\n",
    "print(labeled_data_sets)\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000\n",
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "  \n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)\n",
    "for ex in all_labeled_data.take(5):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "val_data = np.random.random((100, 32))\n",
    "val_labels = np.random.random((100, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/32\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 178.4614 - accuracy: 0.0900 - val_loss: 370.4379 - val_accuracy: 0.1400\n",
      "Epoch 2/32\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 789.0940 - accuracy: 0.1160 - val_loss: 1276.2204 - val_accuracy: 0.0900\n",
      "Epoch 3/32\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 1893.9300 - accuracy: 0.0840 - val_loss: 2265.7230 - val_accuracy: 0.0800\n",
      "Epoch 4/32\n",
      "1000/1000 [==============================] - 0s 99us/sample - loss: 2883.4460 - accuracy: 0.0970 - val_loss: 3304.5038 - val_accuracy: 0.1200\n",
      "Epoch 5/32\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 3217.7243 - accuracy: 0.1100 - val_loss: 3082.2223 - val_accuracy: 0.0700\n",
      "Epoch 6/32\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 3250.8884 - accuracy: 0.0700 - val_loss: 3111.5793 - val_accuracy: 0.1200\n",
      "Epoch 7/32\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 3862.7633 - accuracy: 0.0980 - val_loss: 5008.9267 - val_accuracy: 0.0700\n",
      "Epoch 8/32\n",
      "1000/1000 [==============================] - 0s 71us/sample - loss: 6102.8521 - accuracy: 0.1030 - val_loss: 5464.6501 - val_accuracy: 0.0800\n",
      "Epoch 9/32\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 5532.1652 - accuracy: 0.1110 - val_loss: 9288.0741 - val_accuracy: 0.0800\n",
      "Epoch 10/32\n",
      "1000/1000 [==============================] - 0s 65us/sample - loss: 6617.7998 - accuracy: 0.0970 - val_loss: 5978.0688 - val_accuracy: 0.1200\n",
      "Epoch 11/32\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 8288.0065 - accuracy: 0.1080 - val_loss: 10450.6957 - val_accuracy: 0.0800\n",
      "Epoch 12/32\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 9719.7451 - accuracy: 0.1110 - val_loss: 9422.4080 - val_accuracy: 0.0900\n",
      "Epoch 13/32\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 9227.0708 - accuracy: 0.1060 - val_loss: 13727.8949 - val_accuracy: 0.1400\n",
      "Epoch 14/32\n",
      "1000/1000 [==============================] - 0s 72us/sample - loss: 11478.6735 - accuracy: 0.1020 - val_loss: 9078.0309 - val_accuracy: 0.1200\n",
      "Epoch 15/32\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 8953.3711 - accuracy: 0.0990 - val_loss: 9514.4076 - val_accuracy: 0.1200\n",
      "Epoch 16/32\n",
      "1000/1000 [==============================] - 0s 68us/sample - loss: 10225.0761 - accuracy: 0.1180 - val_loss: 10580.2311 - val_accuracy: 0.0700\n",
      "Epoch 17/32\n",
      "1000/1000 [==============================] - 0s 62us/sample - loss: 13358.1438 - accuracy: 0.0820 - val_loss: 10956.6262 - val_accuracy: 0.0900\n",
      "Epoch 18/32\n",
      "1000/1000 [==============================] - 0s 60us/sample - loss: 13654.3903 - accuracy: 0.0960 - val_loss: 12749.8244 - val_accuracy: 0.1200\n",
      "Epoch 19/32\n",
      "1000/1000 [==============================] - 0s 74us/sample - loss: 14312.7434 - accuracy: 0.0920 - val_loss: 8259.8818 - val_accuracy: 0.1200\n",
      "Epoch 20/32\n",
      "1000/1000 [==============================] - 0s 78us/sample - loss: 15444.8902 - accuracy: 0.0810 - val_loss: 11185.5287 - val_accuracy: 0.0700\n",
      "Epoch 21/32\n",
      "1000/1000 [==============================] - 0s 71us/sample - loss: 15222.3947 - accuracy: 0.0930 - val_loss: 14691.3056 - val_accuracy: 0.0800\n",
      "Epoch 22/32\n",
      "1000/1000 [==============================] - 0s 61us/sample - loss: 18030.1928 - accuracy: 0.0900 - val_loss: 20122.5968 - val_accuracy: 0.0800\n",
      "Epoch 23/32\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 16112.1225 - accuracy: 0.0940 - val_loss: 13033.3204 - val_accuracy: 0.0700\n",
      "Epoch 24/32\n",
      "1000/1000 [==============================] - 0s 73us/sample - loss: 15525.5603 - accuracy: 0.1010 - val_loss: 19706.4152 - val_accuracy: 0.0700\n",
      "Epoch 25/32\n",
      "1000/1000 [==============================] - 0s 72us/sample - loss: 18557.0139 - accuracy: 0.1030 - val_loss: 19926.8148 - val_accuracy: 0.1200\n",
      "Epoch 26/32\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 17413.6258 - accuracy: 0.1170 - val_loss: 13323.8301 - val_accuracy: 0.0800\n",
      "Epoch 27/32\n",
      "1000/1000 [==============================] - 0s 68us/sample - loss: 15547.1686 - accuracy: 0.0970 - val_loss: 13675.0430 - val_accuracy: 0.0700\n",
      "Epoch 28/32\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 19008.8762 - accuracy: 0.1020 - val_loss: 18539.9050 - val_accuracy: 0.1200\n",
      "Epoch 29/32\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 18015.2366 - accuracy: 0.1070 - val_loss: 29340.2256 - val_accuracy: 0.1200\n",
      "Epoch 30/32\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 22655.2396 - accuracy: 0.0990 - val_loss: 21196.1470 - val_accuracy: 0.1200\n",
      "Epoch 31/32\n",
      "1000/1000 [==============================] - 0s 74us/sample - loss: 23143.8080 - accuracy: 0.1140 - val_loss: 26653.8961 - val_accuracy: 0.1200\n",
      "Epoch 32/32\n",
      "1000/1000 [==============================] - 0s 70us/sample - loss: 27256.4902 - accuracy: 0.1050 - val_loss: 13239.4773 - val_accuracy: 0.0700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6ea8217048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "# Add another:\n",
    "# Add a softmax layer with 10 output units:\n",
    "layers.Dense(10, activation='softmax')])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.1),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(data, labels, epochs=32, batch_size=32,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
